{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Train Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Using your baseline model from yesterday, hyperparameter tune it and report on your highest validation accuracy. Your singular goal today is to achieve the highest accuracy possible.\n",
    "\n",
    "*Don't forgot to switch to GPU on Colab!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJ2b3wk62Ud"
   },
   "source": [
    "### Hyperparameters to Tune\n",
    "\n",
    "At a minimum, tune each of these hyperparameters using any strategy we discussed during lecture today: \n",
    "- Optimizer\n",
    "- Learning Rate\n",
    "- Activiation Function\n",
    "  - At least 1 subparameter within the Relu activation function\n",
    "- Number of Neurons in Hidden Layers\n",
    "- Number of Hidden Layers\n",
    "- Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, SGD, Ftrl, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw(path):\n",
    "    \"\"\"\n",
    "    Loads the data, normalizes it, splits into target and feature\n",
    "    Preforms train test split\n",
    "    @param path: path to data\n",
    "    returns : train test split tuple\n",
    "    \"\"\"\n",
    "    data = np.load('../quickdraw10.npz')\n",
    "    X = data['arr_0']\n",
    "    y = data['arr_1']\n",
    "    X = X  / 255\n",
    "    X, y = shuffle(X, y)\n",
    "    return train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw(\"../quickdraw10.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When i did it on my own it worked but using the built ins im getting a memory crash, guess i don't have enough vram. Dunno how to use tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard + Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nuerons:int, hidden_layers:int, activation:str, out_activation, optimizer, learning_rate:float):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # first layer is special so make out of for loop\n",
    "    model.add(Dense(nuerons, input_dim=784, activation=activation))\n",
    "    if hidden_layers > 1:\n",
    "        for i in [int(i) for i in np.linspace(10,nuerons,hidden_layers+1)[1:-1]]:\n",
    "            model.add(Dense(i, activation=activation))\n",
    "    model.add(Dense(10, activation=out_activation))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': (32,64,512),\n",
    "              'epochs': [15],\n",
    "              'nuerons': (500, 200, 32),\n",
    "              'hidden_layers': (1,3,5,8),\n",
    "              'activation': ('relu', 'sigmoid', 'tanh'),\n",
    "              'out_activation': ['softmax'],\n",
    "              'optimizer': (Adam, Adagrad, SGD, Ftrl, RMSprop),\n",
    "              'learning_rate': tuple(np.linspace(.001, .01, 5))\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/parallel.py\", line 253, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/parallel.py\", line 253, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 514, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 223, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 157, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"<ipython-input-4-c5c52bace5eb>\", line 3, in create_model\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 117, in __init__\n    name=name, autocast=False)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 308, in __init__\n    self._init_batch_counters()\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 317, in _init_batch_counters\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\n    return cls._variable_v2_call(*args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 256, in _variable_v2_call\n    shape=shape)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2646, in default_variable_creator_v2\n    shape=shape)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1518, in __init__\n    distribute_strategy=distribute_strategy)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1652, in _init_from_args\n    name=\"initial_value\", dtype=dtype)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\n    return constant_op.constant(value, dtype, name=name)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n    allow_broadcast=True)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\n    ctx.ensure_initialized()\n  File \"/home/allan/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f29b81a7c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/u4-s2-giZ6Z0KA/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "search = RandomizedSearchCV(estimator=model,param_distributions=param_grid,n_jobs=8,cv=2,random_state=42,verbose=1,n_iter=30)\n",
    "search_result = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {searcg_result.best_score_} using {search_result.best_params_}\")\n",
    "means = search_result.cv_results_['mean_test_score']\n",
    "stds = search_result.cv_results_['std_test_score']\n",
    "params = search_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.0032, 0.0055, 0.0078, 0.01]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(i,4) for i in  np.linspace(.001, .01, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try keras classifier i think that it inherit the parameters from the function used to amake the model\n",
    "# if yes you can grid search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I didn't want to more or less copy nd paste so i did this, waste of time i think, was what i did 1st "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # possible inputs for model creation\n",
    "# # number of nuerons in a layer\n",
    "# # number of layers\n",
    "# # activation function per layer\n",
    "# # loss function\n",
    "# # optimizer\n",
    "\n",
    "# # layer_schema = ((nuerons, activation), (), ())\n",
    "# # optimzer:str check if in tensorflow else raise error\n",
    "# # loss:str check if in tf else raise error\n",
    "\n",
    "# def make_model(layer_schema:tuple, optimizer, learning_rate:float):\n",
    "#     \"\"\"\n",
    "#     Create's a densly connected sequential nueral network given the schema\n",
    "#     @param layer_schema: multi-dimensional tuple where each elemnt is a layer and the sub-elements[0] \n",
    "#     are the number of nuerons for the layer, and the sub-elements[1] are the activation function the layer\n",
    "#     ex: if layer_schema == ((32, 'relu'), (50, 'relu'), (10, 'softmax')) there are 4 layer.\n",
    "#     The implicit input layer, 2 hidden layers, and 1 output layer. The 1st hidden layer has 32 nuerons and and acticvation function of 'relu',\n",
    "#     second has 50 nuerons and 'relu', and the an output layer with 10 nuerons and 'softmax' as the activation\n",
    "#     @param optimzer: non instantiated optimizer\n",
    "#     @param learning_rate: float, learning rate for the optimizer\n",
    "#     \"\"\"\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     # the first layer has an extra field input so it's done out of the for loop\n",
    "#     model.add(Dense(layer_schema[0][0], input_dim=784, activation=layer_schema[0][1]))\n",
    "#     for layer in layer_schema[1:]:\n",
    "#         model.add(Dense(layer[0], activation=layer[1]))\n",
    "#     #compile model\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # how to hypertune?\n",
    "# # [optimizers], [schemas], [loss but constant]\n",
    "# # for optimizer in optimizers\n",
    "# #     for schema in schemas\n",
    "# #         for loss in loss\n",
    "# #             call make given these hyper params\n",
    "# #             fit model\n",
    "# #             log something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers = [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD]\n",
    "# schemas = [\n",
    "#     ((32, 'relu'), (10, 'softmax')), \n",
    "#     ((32, 'relu'), (20, 'relu'), (10, \"softmax\")),\n",
    "#     ((32, 'tanh'), (10, 'softmax')), \n",
    "#     ((32, 'tanh'), (20, 'tanh'), (10, \"softmax\"))\n",
    "\n",
    "# ]\n",
    "# rates = [.001, .01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8431 - accuracy: 0.7513\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6492 - accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5926 - accuracy: 0.8274\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5595 - accuracy: 0.8377\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5361 - accuracy: 0.8442\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5156 - accuracy: 0.8502\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5027 - accuracy: 0.8537\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4889 - accuracy: 0.8569\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4786 - accuracy: 0.8600\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4697 - accuracy: 0.8630\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4620 - accuracy: 0.8645\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4546 - accuracy: 0.8667\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4487 - accuracy: 0.8680\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4420 - accuracy: 0.8701\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4369 - accuracy: 0.8723\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.5787 - accuracy: 0.8366\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.7833 - accuracy: 0.7629\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6749 - accuracy: 0.7974\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6440 - accuracy: 0.8071\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6277 - accuracy: 0.8099\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6190 - accuracy: 0.8137\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6107 - accuracy: 0.8161\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6029 - accuracy: 0.8182\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5975 - accuracy: 0.8200\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5921 - accuracy: 0.8221\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5897 - accuracy: 0.8234\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5800 - accuracy: 0.8241\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5793 - accuracy: 0.8245\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5760 - accuracy: 0.8258\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5705 - accuracy: 0.8278\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5670 - accuracy: 0.8284\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.7379 - accuracy: 0.7943\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8371 - accuracy: 0.7465\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6322 - accuracy: 0.8138\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5819 - accuracy: 0.8267\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5493 - accuracy: 0.8355\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5237 - accuracy: 0.8427\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5059 - accuracy: 0.8469\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4906 - accuracy: 0.8524\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4786 - accuracy: 0.8560\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4681 - accuracy: 0.8593\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4590 - accuracy: 0.8619\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4494 - accuracy: 0.8643\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4425 - accuracy: 0.8659\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4354 - accuracy: 0.8680\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4284 - accuracy: 0.8713\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4232 - accuracy: 0.8719\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.5770 - accuracy: 0.8333\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7945 - accuracy: 0.7571\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6774 - accuracy: 0.7956\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6521 - accuracy: 0.8051\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6324 - accuracy: 0.8094\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6223 - accuracy: 0.8146\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6089 - accuracy: 0.8182\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6026 - accuracy: 0.8199\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5918 - accuracy: 0.8234\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5849 - accuracy: 0.8249\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5811 - accuracy: 0.8266\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5758 - accuracy: 0.8273\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5722 - accuracy: 0.8290\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5660 - accuracy: 0.8302\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5633 - accuracy: 0.8327\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5591 - accuracy: 0.8323\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.6737 - accuracy: 0.8071\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8313 - accuracy: 0.7514\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6367 - accuracy: 0.8134\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5802 - accuracy: 0.8280\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5466 - accuracy: 0.8386\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5255 - accuracy: 0.8437\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5084 - accuracy: 0.8492\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4953 - accuracy: 0.8524\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4835 - accuracy: 0.8571\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4733 - accuracy: 0.8592\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4656 - accuracy: 0.8618\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4575 - accuracy: 0.8643\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4518 - accuracy: 0.8660\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4462 - accuracy: 0.8669\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4392 - accuracy: 0.8695\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4346 - accuracy: 0.8709\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.6023 - accuracy: 0.8265\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8068 - accuracy: 0.7523\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7174 - accuracy: 0.7812\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7026 - accuracy: 0.7861\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6817 - accuracy: 0.7928\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6703 - accuracy: 0.7956\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6630 - accuracy: 0.7979\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6493 - accuracy: 0.8006\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6512 - accuracy: 0.8021\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6433 - accuracy: 0.8036\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6316 - accuracy: 0.8102\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6312 - accuracy: 0.8093\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6260 - accuracy: 0.8094\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6190 - accuracy: 0.8126\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6132 - accuracy: 0.8156\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6095 - accuracy: 0.8163\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.6889 - accuracy: 0.7973\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8423 - accuracy: 0.7479\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6211 - accuracy: 0.8135\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5584 - accuracy: 0.8326\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5241 - accuracy: 0.8423\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5019 - accuracy: 0.8491\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4849 - accuracy: 0.8529\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4698 - accuracy: 0.8582\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4592 - accuracy: 0.8606\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4494 - accuracy: 0.8634\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4408 - accuracy: 0.8668\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4324 - accuracy: 0.8693\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4261 - accuracy: 0.8704\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4202 - accuracy: 0.8723\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4153 - accuracy: 0.8746\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.4085 - accuracy: 0.8770\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.5858 - accuracy: 0.8297\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8208 - accuracy: 0.7438\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7444 - accuracy: 0.7726\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7248 - accuracy: 0.7790\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7140 - accuracy: 0.7812\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6998 - accuracy: 0.7855\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6916 - accuracy: 0.7881\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6871 - accuracy: 0.7909\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6824 - accuracy: 0.7918\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6801 - accuracy: 0.7915\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6708 - accuracy: 0.7967\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6708 - accuracy: 0.7946\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6673 - accuracy: 0.7972\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6634 - accuracy: 0.7983\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6582 - accuracy: 0.7981\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6583 - accuracy: 0.8001\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.7040 - accuracy: 0.7895\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.9866 - accuracy: 0.3520\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.4711 - accuracy: 0.5507\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.2409 - accuracy: 0.6200\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.1260 - accuracy: 0.6569\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.0564 - accuracy: 0.6829\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.0094 - accuracy: 0.6999\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9758 - accuracy: 0.7115\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9504 - accuracy: 0.7201\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9304 - accuracy: 0.7270\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9139 - accuracy: 0.7326\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9000 - accuracy: 0.7375\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8880 - accuracy: 0.7419\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8773 - accuracy: 0.7452\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8677 - accuracy: 0.7482\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8589 - accuracy: 0.7510\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.8696 - accuracy: 0.7469\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.1573 - accuracy: 0.6535\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8774 - accuracy: 0.7466\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8114 - accuracy: 0.7659\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7669 - accuracy: 0.7791\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.7314 - accuracy: 0.7896\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.7019 - accuracy: 0.7978\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6767 - accuracy: 0.8054\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6554 - accuracy: 0.8113\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6363 - accuracy: 0.8168\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6200 - accuracy: 0.8211\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6056 - accuracy: 0.8257\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5928 - accuracy: 0.8286\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5814 - accuracy: 0.8323\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5715 - accuracy: 0.8350\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5623 - accuracy: 0.8374\n",
      "1032/1032 [==============================] - 1s 973us/step - loss: 0.6054 - accuracy: 0.8234\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 2.1672 - accuracy: 0.2260\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.7687 - accuracy: 0.4242\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.3571 - accuracy: 0.5973\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.1646 - accuracy: 0.6531\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.0637 - accuracy: 0.6844\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.0013 - accuracy: 0.7038\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9590 - accuracy: 0.7184\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9278 - accuracy: 0.7267\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9037 - accuracy: 0.7338\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8840 - accuracy: 0.7394\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8672 - accuracy: 0.7445\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8525 - accuracy: 0.7482\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8398 - accuracy: 0.7525\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8282 - accuracy: 0.7561\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8175 - accuracy: 0.7595\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.8263 - accuracy: 0.7555\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.2501 - accuracy: 0.6019\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8451 - accuracy: 0.7506\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7640 - accuracy: 0.7759: 0s - loss: 0.7650 - accu\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7114 - accuracy: 0.7915\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6718 - accuracy: 0.8034\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6420 - accuracy: 0.8123\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6171 - accuracy: 0.8190\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5962 - accuracy: 0.8254\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5794 - accuracy: 0.8300\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5643 - accuracy: 0.8345\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5525 - accuracy: 0.8377\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5414 - accuracy: 0.8413\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5313 - accuracy: 0.8446\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5219 - accuracy: 0.8460\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5129 - accuracy: 0.8497\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.5744 - accuracy: 0.8316\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.8428 - accuracy: 0.4021\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.3624 - accuracy: 0.6059\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.2018 - accuracy: 0.6475\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.1157 - accuracy: 0.6707\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.0591 - accuracy: 0.6863\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.0179 - accuracy: 0.6979\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9862 - accuracy: 0.7073\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9609 - accuracy: 0.7144\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9402 - accuracy: 0.7209\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9227 - accuracy: 0.7259\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.9078 - accuracy: 0.7301\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8947 - accuracy: 0.7346\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8832 - accuracy: 0.7374\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8729 - accuracy: 0.7410\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8635 - accuracy: 0.7444\n",
      "1032/1032 [==============================] - 1s 991us/step - loss: 0.8670 - accuracy: 0.7432\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 1.1564 - accuracy: 0.6511\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8702 - accuracy: 0.7424\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.8039 - accuracy: 0.7621\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.7620 - accuracy: 0.7761\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.7297 - accuracy: 0.7870\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7038 - accuracy: 0.7942\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6821 - accuracy: 0.8008\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6633 - accuracy: 0.8062\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6472 - accuracy: 0.8108\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6335 - accuracy: 0.8141\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.6210 - accuracy: 0.8181\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6096 - accuracy: 0.8220\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5997 - accuracy: 0.8244\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5904 - accuracy: 0.8276\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 2s 1ms/step - loss: 0.5819 - accuracy: 0.8296\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.6183 - accuracy: 0.8172\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.9215 - accuracy: 0.3920\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.4898 - accuracy: 0.5892\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.2992 - accuracy: 0.6411\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.1838 - accuracy: 0.6680\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.1054 - accuracy: 0.6850\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.0490 - accuracy: 0.6981\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.0063 - accuracy: 0.7084\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9728 - accuracy: 0.7171\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9456 - accuracy: 0.7236\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9230 - accuracy: 0.7288\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.9037 - accuracy: 0.7345\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8869 - accuracy: 0.7390\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8722 - accuracy: 0.7433\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8589 - accuracy: 0.7473\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8469 - accuracy: 0.7506\n",
      "1032/1032 [==============================] - 1s 1ms/step - loss: 0.8503 - accuracy: 0.7467\n",
      "Epoch 1/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 1.1780 - accuracy: 0.6417\n",
      "Epoch 2/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.8487 - accuracy: 0.7481\n",
      "Epoch 3/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7682 - accuracy: 0.7732\n",
      "Epoch 4/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.7181 - accuracy: 0.7886\n",
      "Epoch 5/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6804 - accuracy: 0.7999\n",
      "Epoch 6/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6515 - accuracy: 0.8077\n",
      "Epoch 7/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6271 - accuracy: 0.8163\n",
      "Epoch 8/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.6060 - accuracy: 0.8221\n",
      "Epoch 9/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5881 - accuracy: 0.8280\n",
      "Epoch 10/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5723 - accuracy: 0.8327\n",
      "Epoch 11/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5585 - accuracy: 0.8366\n",
      "Epoch 12/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5469 - accuracy: 0.8394\n",
      "Epoch 13/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5358 - accuracy: 0.8427\n",
      "Epoch 14/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5259 - accuracy: 0.8452\n",
      "Epoch 15/15\n",
      "2094/2094 [==============================] - 3s 1ms/step - loss: 0.5170 - accuracy: 0.8477\n",
      "1032/1032 [==============================] - 1s 994us/step - loss: 0.5707 - accuracy: 0.8307\n"
     ]
    }
   ],
   "source": [
    "# for optimizer in optimizers:\n",
    "#     for schema in schemas:\n",
    "#         for rate in rates:\n",
    "#             model = make_model(layer_schema=schema, learning_rate=rate, optimizer=optimizer)\n",
    "#             model.fit(X_train, y_train, epochs=15)\n",
    "#             with open('assignment_logs.txt', 'a') as logs:\n",
    "#                 logs.write(f'{model.evaluate(X_test, y_test)[-1]},{\"adam\" if type(optimizer()) == tf.python.keras.optimizer_v2.adam.Adam else \"sgd\"},{rate},{len(schema) + 1},{\",\".join(str(i[0]) for i in schema)},{\",\".join([i[1] for i in schema])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"assignment_logs.txt\", \"r\") as logs:\n",
    "#     data = logs.readlines()\n",
    "#     data = [i.strip('\\n').split(',') for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(range(1,9), [float(i[0]) for i in data[:8]], color='r', label='adam')\n",
    "# ax.scatter(range(9,17), [float(i[0]) for i in data[8:]], color='b', label='sgd')\n",
    "\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.8365757465362549', 'adam', '0.001', '3', '32', '10', 'relu', 'softmax'],\n",
       " ['0.7942727208137512', 'adam', '0.01', '3', '32', '10', 'relu', 'softmax'],\n",
       " ['0.8333333134651184',\n",
       "  'adam',\n",
       "  '0.001',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'relu',\n",
       "  'relu',\n",
       "  'softmax'],\n",
       " ['0.8070605993270874',\n",
       "  'adam',\n",
       "  '0.01',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'relu',\n",
       "  'relu',\n",
       "  'softmax'],\n",
       " ['0.8264545202255249', 'adam', '0.001', '3', '32', '10', 'tanh', 'softmax'],\n",
       " ['0.7973030209541321', 'adam', '0.01', '3', '32', '10', 'tanh', 'softmax'],\n",
       " ['0.829727292060852',\n",
       "  'adam',\n",
       "  '0.001',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'tanh',\n",
       "  'tanh',\n",
       "  'softmax'],\n",
       " ['0.7894545197486877',\n",
       "  'adam',\n",
       "  '0.01',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'tanh',\n",
       "  'tanh',\n",
       "  'softmax'],\n",
       " ['0.7469090819358826', 'sgd', '0.001', '3', '32', '10', 'relu', 'softmax'],\n",
       " ['0.8233939409255981', 'sgd', '0.01', '3', '32', '10', 'relu', 'softmax'],\n",
       " ['0.7554848194122314',\n",
       "  'sgd',\n",
       "  '0.001',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'relu',\n",
       "  'relu',\n",
       "  'softmax'],\n",
       " ['0.8315757513046265',\n",
       "  'sgd',\n",
       "  '0.01',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'relu',\n",
       "  'relu',\n",
       "  'softmax'],\n",
       " ['0.7431514859199524', 'sgd', '0.001', '3', '32', '10', 'tanh', 'softmax'],\n",
       " ['0.8172121047973633', 'sgd', '0.01', '3', '32', '10', 'tanh', 'softmax'],\n",
       " ['0.746666669845581',\n",
       "  'sgd',\n",
       "  '0.001',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'tanh',\n",
       "  'tanh',\n",
       "  'softmax'],\n",
       " ['0.8307272791862488',\n",
       "  'sgd',\n",
       "  '0.01',\n",
       "  '4',\n",
       "  '32',\n",
       "  '20',\n",
       "  '10',\n",
       "  'tanh',\n",
       "  'tanh',\n",
       "  'softmax']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of log file\n",
    "# accuracyoptimizer,learning_rate,num_layers,nuerons,activation\n",
    "# 98.32,adam,.001,4,784,32,50,10,None,relu,relu,softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for optimizer in optimizers:\n",
    "#     for schema in schemas:\n",
    "#         for rate in rates:\n",
    "#             print(f'{model.evaluate(X_test, y_test)[-1]}')\n",
    "#             print(f'{\"adam\" if type(optimizer) == tf.python.keras.optimizer_v2.adam.Adam else \"sgd\"}')\n",
    "#             print(f'{rate}')\n",
    "#             print(f'{len(schema) + 1}')\n",
    "#             print(f'{\",\".join(str(i[0]) for i in schema)}')\n",
    "#             print(f'{\",\".join(str(i[1]) for i in schema)}')\n",
    "#             with open(\"assignment_logs.txt\", 'a') as logs:\n",
    "#                 logs.write(f'{model.evaluate(X_test, y_test)[-1]},{\"adam\" if type(optimizer) == tf.python.keras.optimizer_v2.adam.Adam else \"sgd\"},{rate},{len(schema) + 1},{\",\".join(str(i[0]) for i in schema)},{\",\".join([i[1] for i in schema])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKbr1gRg9BXs"
   },
   "source": [
    "### Stretch Goals\n",
    "- Implement Bayesian Hyper-parameter Optimization\n",
    "- Select a new dataset and apply a neural network to it.\n",
    "- Use a cloud base experiment tracking framework such as weights and biases\n",
    "- Research potential architecture ideas for this problem. Try Lenet-10 for example. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
